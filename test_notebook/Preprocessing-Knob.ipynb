{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497a5903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "sys.path.append('../')\n",
    "from models.knobs import rocksdb_knobs_make_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a34fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_internal = \"internal\"\n",
    "PATH_external = \"external\"\n",
    "PATH_knobs = \"rocksdb_conf\"\n",
    "wk_len = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c05bc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knobs = rocksdb_knobs_make_dict(PATH_knobs)\n",
    "knobs = pd.DataFrame(data=knobs['data'].astype(np.float32), columns=knobs['columnlabels'])\n",
    "columns = knobs.columns\n",
    "knobs.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1bddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KNOB NAME \\t\\t\\t\\t sum of value counts\")\n",
    "for col in columns:\n",
    "    print(f\"{col:40}\\t{len(knobs[[col]].value_counts())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e77d639",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index_value = dict()\n",
    "for col in columns:\n",
    "    iv = knobs[[col]].value_counts()#.reset_index(level=0).drop(columns=0)\n",
    "    iv = iv.sort_index()\n",
    "    index_value[col] = pd.Series(data=range(len(iv)), index=iv.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec1b459",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0712d7f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knobs_one_hot = torch.Tensor()\n",
    "for i in tqdm(range(len(knobs))):   \n",
    "    sample = torch.Tensor()\n",
    "    for col in columns:\n",
    "        knob_one_hot = torch.zeros(len(index_value[col]))\n",
    "        knob_one_hot[index_value[col][knobs[col][i]]] = 1\n",
    "        sample = torch.cat((sample, knob_one_hot))\n",
    "    sample = sample.unsqueeze(0)\n",
    "    knobs_one_hot = torch.cat((knobs_one_hot, sample))\n",
    "knobs_one_hot.shape\n",
    "np.save('knobsOneHot.npy', np.array(knobs_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c819e17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.load('knobsOneHot.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_internal = \"internal\"\n",
    "PATH_external = \"external\"\n",
    "wk_len = 16\n",
    "internal_dict = {}\n",
    "\n",
    "pruned_im = pd.read_csv(os.path.join(PATH_internal, 'internal_ensemble_pruned_tmp.csv'), index_col=0)\n",
    "for wk in range(wk_len):\n",
    "    im = pd.read_csv(os.path.join(PATH_internal, f'internal_results_{wk}.csv'), index_col=0)\n",
    "    internal_dict[wk] = im[pruned_im.columns]\n",
    "    break\n",
    "internal_dict[0].head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_dict = {}\n",
    "for wk in range(wk_len):\n",
    "    ex = pd.read_csv(os.path.join(PATH_external, f'external_results_{wk}.csv'), index_col=0)\n",
    "    external_dict[wk] = ex\n",
    "    break\n",
    "external_dict[0].head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa739556",
   "metadata": {},
   "source": [
    "## Test Train\n",
    "- train: 16000, test: 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e90336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd7743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocksDBDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(RocksDBDataset, self).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx], self.y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ea44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SingleNet, self).__init__()\n",
    "        self.input_dim = input_dim # 3327\n",
    "        self.hidden_dim = hidden_dim # 1024\n",
    "        self.output_dim = output_dim # 148\n",
    "        self.knob_fc = nn.Sequential(nn.Linear(self.input_dim, self.hidden_dim), nn.ReLU())\n",
    "#         self.hidden = nn.Sequential(nn.Linear(self.hidden_dim, 64), nn.ReLU())\n",
    "        self.im_fc = nn.Sequential(nn.Linear(self.hidden_dim, self.output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_kb = self.knob_fc(x)\n",
    "#         self.h = self.hidden(self.x_kb)\n",
    "        self.x_im = self.im_fc(self.x_kb)\n",
    "        return self.x_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e042cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, lr):\n",
    "    ## Construct optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    ## Set phase\n",
    "    model.train()\n",
    "    \n",
    "    ## Train start\n",
    "    total_loss = 0.\n",
    "    for data, target in train_loader:\n",
    "        ## data.shape = (batch_size, 22)\n",
    "        ## target.shape = (batch_size, 1)\n",
    "        ## initilize gradient\n",
    "        optimizer.zero_grad()\n",
    "        ## predict\n",
    "        output = model(data) # output.shape = (batch_size, 1)\n",
    "        ## loss\n",
    "        loss = F.mse_loss(output, target)\n",
    "        ## backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ## Logging\n",
    "        total_loss += loss.item()\n",
    "    total_loss /= len(train_loader)\n",
    "    return total_loss\n",
    "\n",
    "def valid(model, valid_loader):\n",
    "    ## Set phase\n",
    "    model.eval()\n",
    "    \n",
    "    ## Valid start    \n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for data, target in valid_loader:\n",
    "            output = model(data)\n",
    "            loss = F.mse_loss(output, target) # mean squared error\n",
    "            total_loss += loss.item()\n",
    "    total_loss /= len(valid_loader)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed8b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "knobOneHot = np.load('knobsOneHot.npy')\n",
    "internal_m = internal_dict[0]\n",
    "external_m = external_dict[0]\n",
    "knobOneHot.shape, internal_m.shape, external_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, Im_tr, Im_te, y_tr, y_te, knob_tr, knob_te = \\\n",
    "            train_test_split(knobOneHot, internal_m, external_m, knobs, test_size=0.2, random_state=24)\n",
    "X_tr.shape, X_te.shape, Im_tr.shape, Im_te.shape, y_tr.shape, y_te.shape, knob_tr.shape, knob_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64ab563",
   "metadata": {},
   "source": [
    "### Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3636eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_X = MinMaxScaler().fit(X_tr) # range: 0~1\n",
    "scaler_knob = MinMaxScaler().fit(knob_tr)\n",
    "scaler_Im = MinMaxScaler().fit(Im_tr)\n",
    "scaler_y = StandardScaler().fit(y_tr)\n",
    "\n",
    "# X_norm_tr = torch.Tensor(scaler_X.transform(X_tr)).cuda()\n",
    "# X_norm_te = torch.Tensor(scaler_X.transform(X_te)).cuda()\n",
    "X_tr = torch.Tensor(X_tr).cuda()\n",
    "X_te = torch.Tensor(X_te).cuda()\n",
    "Im_norm_tr = torch.Tensor(scaler_Im.transform(Im_tr)).cuda()\n",
    "Im_norm_te = torch.Tensor(scaler_Im.transform(Im_te)).cuda()\n",
    "y_norm_tr = torch.Tensor(scaler_y.transform(y_tr)).cuda()\n",
    "y_norm_te = torch.Tensor(scaler_y.transform(y_te)).cuda()\n",
    "\n",
    "Dataset_tr = RocksDBDataset(X_tr, Im_norm_tr)\n",
    "Dataset_te = RocksDBDataset(X_te, Im_norm_te)\n",
    "\n",
    "loader_tr = DataLoader(dataset = Dataset_tr, batch_size = 32, shuffle=True)\n",
    "loader_te = DataLoader(dataset = Dataset_te, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9afcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "k2i_model = SingleNet(input_dim=X_tr.shape[1], hidden_dim=1024, output_dim=148).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a998a4a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 30\n",
    "losses_tr = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_tr = train(k2i_model, loader_tr, lr)\n",
    "    losses_tr.append(loss_tr)\n",
    "\n",
    "    print(f\"[{epoch:02d}/{epochs}] loss_tr: {loss_tr}\")\n",
    "        \n",
    "# print(f\"[{epoch:02d}/{epochs}] loss_tr: {loss_tr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922ffb5b",
   "metadata": {},
   "source": [
    "### Train with knob2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad7e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_table = k2i_model.knob_fc[0].weight.T.cpu().detach().numpy()\n",
    "lookup_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knob2vec(data, table):\n",
    "    k2vec = np.zeros((data.shape[0], 22, table.shape[1]))\n",
    "    for i in range(data.shape[0]):\n",
    "        idx = (data[i]==1).nonzero().squeeze().cpu().detach().numpy()\n",
    "        k2vec[i] = lookup_table[idx]\n",
    "    return k2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d46b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "K2vec_tr = torch.Tensor(get_knob2vec(X_tr, lookup_table)).cuda()\n",
    "K2vec_tr = torch.reshape(K2vec_tr, (K2vec_tr.shape[0], -1))\n",
    "K2vec_te = torch.Tensor(get_knob2vec(X_te, lookup_table)).cuda()\n",
    "K2vec_te = torch.reshape(K2vec_te, (K2vec_te.shape[0], -1))\n",
    "\n",
    "Dataset_K2vec_tr = RocksDBDataset(K2vec_tr, y_norm_tr)\n",
    "Dataset_K2vec_te = RocksDBDataset(K2vec_te, y_norm_te)\n",
    "\n",
    "loader_K2vec_tr = DataLoader(dataset = Dataset_K2vec_tr, batch_size = 32, shuffle=True)\n",
    "loader_K2vec_te = DataLoader(dataset = Dataset_K2vec_te, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86605f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SingleNet(input_dim=K2vec_tr.shape[-1], hidden_dim=64, output_dim=4).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a6bff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 30\n",
    "losses_tr = []\n",
    "losses_te = []\n",
    "for epoch in range(epochs):\n",
    "    loss_tr = train(model, loader_K2vec_tr, lr)\n",
    "    loss_te = valid(model, loader_K2vec_te)\n",
    "    \n",
    "    losses_tr.append(loss_tr)\n",
    "    losses_te.append(loss_te)\n",
    "    \n",
    "    print(f\"[{epoch:02d}/{epochs}] loss_tr: {loss_tr}\\tloss_te:{loss_te:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c1edeb",
   "metadata": {},
   "source": [
    "### Train with raw knob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3857a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "knob_norm_tr = torch.Tensor(scaler_knob.transform(knob_tr)).cuda()\n",
    "knob_norm_te = torch.Tensor(scaler_knob.transform(knob_te)).cuda()\n",
    "\n",
    "Dataset_knob_tr = RocksDBDataset(knob_norm_tr, y_norm_tr)\n",
    "Dataset_knob_te = RocksDBDataset(knob_norm_te, y_norm_te)\n",
    "\n",
    "loader_knob_tr = DataLoader(dataset = Dataset_knob_tr, batch_size = 32, shuffle=True)\n",
    "loader_knob_te = DataLoader(dataset = Dataset_knob_te, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e93d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "knob_model = SingleNet(input_dim=knob_norm_tr.shape[1], hidden_dim=16, output_dim=4).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec51bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 30\n",
    "losses_tr = []\n",
    "losses_te = []\n",
    "for epoch in range(epochs):\n",
    "    loss_tr = train(knob_model, loader_knob_tr, lr)\n",
    "    loss_te = valid(knob_model, loader_knob_te)\n",
    "    \n",
    "    losses_tr.append(loss_tr)\n",
    "    losses_te.append(loss_te)\n",
    "    \n",
    "    print(f\"[{epoch:02d}/{epochs}] loss_tr: {loss_tr}\\tloss_te:{loss_te:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c921e7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(scaler_y.inverse_transform(y_norm_te.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d626e2b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(scaler_y.inverse_transform(model(K2vec_te).cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c6404",
   "metadata": {},
   "outputs": [],
   "source": [
    "_30 = scaler_y.inverse_transform(knob_model(knob_norm_te).cpu().detach().numpy()) # 30\n",
    "_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2664c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d8b4d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(scaler_y.inverse_transform(knob_model(knob_norm_te).cpu().detach().numpy())) # 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd00179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[py3.7]",
   "language": "python",
   "name": "py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
